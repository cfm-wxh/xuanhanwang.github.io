---
layout: default
title: Xuanhan Wang, UESTC
---

		<div class="blurb">
			<h3>Dr. Xuanhan Wang (王轩瀚) from UESTC!</h3>
			<p><img src="1.png" alt="photo" style="width:190px;height:190px;float:left;margin-right:35px;">
				<ul class="contacts">
					<li> School of Computer Science and Engineering</li>
					<li>University of Electronic Science and Technology of China</li>
					<li>2006 Xiyuan Ave., West Hi-Tech Zone, Chengdu, China </li>
					<li>Innovation center, Qingshuihe Campus</li>
					<li> <b>LAB</b>: Center for Future Media</li>
					<li> <b>Email</b>: wxuanhan@hotmail.com </li>
					<li><a href="https://scholar.google.com/citations?user=zsm2dpYAAAAJ&hl=en"><div style="color:blue;"> Google Scholar Citation </div></a></li>
					<li><a href="https://stoa-xh91.github.io/"><iv style="color:blue;">Personal Page </div> </a></li>
				</ul>
			</p>
			</div> 

<div class="about">
	
	<p>I'm a PhD student at School of Computer Science and Engineering, University of Electronic Science and Technology of China (UESTC), under the supervision of&nbsp;</font>
	<a href="https://lianligao.github.io/">
	<font color="#0000ff">Prof. Lianli Gao</font></a>
	&nbsp;and &nbsp;<a href="https://lianligao.github.io/"><font color="#0000ff">Prof. Jingkuan Song</font></a>. 
	I obtained my Master degree in Computer Science from UESTC in 2017, under the supervision of&nbsp;</font> 
	<a href="https://lianligao.github.io/">
	<font color="#0000ff">Prof. Lianli Gao</font></a>.&nbsp; I&nbsp;received my BS degree in Software Engineering from UESTC, in 2014.&nbsp;</font></p>

	<p><font color="#000000"><font>My research interet includes large-scale human-centric recognition, such as human pose estimation, human surface understanding, human action recognition, and the related practical applications etc.&nbsp; Specifically, I am mainly focusing on human pose estimation&nbsp;for Human-centric Visual Content Understanding.&nbsp;</font></font></p>

<h4> Recent News</h4>
	<ul>
	<li>[2020/07/25] 1 paper accepted by ACM MM 2020 </li>
	<li>[2020/05/25] 3 papers submitted to ACM MM 2020 </li>
	<li>[2020/01/25] 1 paper submitted to TPAMI 2020</li>
	<li>[2019/05/25] 2 papers accepted by ACM MM 2019</li>
	</ul>

<h4>Working Experience</h4>
<ul>
	<li>PhD student, School of Computer Science and Engineering,&nbsp;<font color="#0000ff">UESTC</font><font color="#000000">,&nbsp;2017.09-Present</font></li>
	<li><font><font color="#000000">Algorithm Engineer,&nbsp;Institute of HIKVISION Research, </font><font color="#0000ff">HIKVISION</font><font color="#000000">,&nbsp;</font></font><font color="#000000" >2017-2019</font></li>
</ul>


<h4>Codes</h4>
	<ul>
		<li><font ><font color="#000000">2017-04-02: We release the code and model for Fused GRU (Video Caption) [</font><a href="https://github.com/wangxuanhan/stafg-capgen-vid" rel="nofollow"><font color="#9900ff">code</font></a><font color="#000000">] [Archive]</font></font></li>
		<li><font ><font color="#000000">2017-04-01: We release the code and model for Deep Region Hashing, (Image Retrieval) [</font><a href="https://github.com/hetaoconquer/DRH" rel="nofollow"><font color="#9900ff">code</font></a><font color="#000000">] [<a href="https://arxiv.org/abs/1701.07901" rel="nofollow">Archive</a>]</font></font></li>
		<li><font ><font color="#000000">2017-03-22: We release&nbsp;the code and model for aLSTM (Video Caption), [</font><a href="https://github.com/zhaoluffy/aLSTMs" rel="nofollow"><font color="#9900ff">code</font></a><font color="#000000">] [<a href="http://dl.acm.org/citation.cfm?id=2967242" rel="nofollow">paper</a>]</font></font></li>
		<li><font ><font color="#000000">2017-03-22. We release the code and model for hLSTMat (Video Caption), [</font><a href="https://github.com/zhaoluffy/hLSTMat" rel="nofollow"><font color="#9900ff">code</font></a><font color="#000000">] [paper]</font></font></li>
	</ul>
	
<h4> Call for Papers</h4>
	<ul>
		<li><b style="color:rgb(255,0,0);">[CFP] </b><font color="#000000"><span>Special Issue:&nbsp;</span><font>Integrating Vision and Language for Semantic Knowledge&nbsp;</font><span>Reasoning and Transfer,&nbsp;</span></font><span><font color="#000000">Paper Submission:<b> August 1, 2019.&nbsp;</b></font><font color="#ff0000" style="font-weight:bold">[<a href="https://www.journals.elsevier.com/journal-of-visual-communication-and-image-representation/call-for-papers/integrating-vision-and-language-for-semantic-knowledge" rel="nofollow">Link</a>]</font></span></li>
	</ul>
	

	

</div>
